#mywork


- Cours de hugging face
	- https://huggingface.co/course/chapter1/1?fw=tf
	-
- Une intro complète
	- [AI-HUB-Deep-Learning-Fundamental/start-machine-learning: A complete guide to start and improve in machine learning (ML), artificial intelligence (AI) in 2022 without ANY background in the field and stay up-to-date with the latest news and state-of-the-art techniques!](https://github.com/AI-HUB-Deep-Learning-Fundamental/start-machine-learning)
	- https://deeplearning.neuromatch.io/tutorials/intro.html
-
- cours d'université ou ressources académiques
	- Programmig A2Z https://shiffman.net/a2z/
	- https://web.stanford.edu/~jurafsky/slp3/
	- https://www.youtube.com/playlist?list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ
	- https://ml4a.github.io/classes/itp-F19/
	- [[Interactive Fiction Class]]
	- https://github.com/cs230-stanford/website-fall-2020/blob/master/past-projects.md
-
- super articles
	- https://github.com/christianversloot/machine-learning-articles
- - la question est de savoir si un réseau de neurones peut faire n'importe quoi comme fonction. Certains papiers le disent. Mais en fait en créeant un réseau de qqs milliers de neurones, avec Relu comme activation, on arrive assez bien à approximer $$x^2 $$ mais sinus et tan c'est pas terrible
	- [[can-neural-networks-approximate-mathematical-functions]]
- pour le batch size en fait, plus j'ai d'exemples plus ça me donne des informations sur la bonne direction à prendre pour faire la descente de gradient. Donc en fait **grand batch size donne meilleur optimal mais ça peut prendre du temps vs mini batch size qui va vite mais si ça se trouve ne va pas convergger **