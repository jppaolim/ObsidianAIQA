{'text': '\n\nDanger pour la «\xa0culture humaine\xa0»\n\nOr «\xa0*le langage est l’étoffe dont presque toute la culture humaine est faite*, poursuit Harari*. Les droits humains, par exemple, ne sont pas inscrits dans notre ADN. Ce sont des artefacts culturels que nous avons créés en nous racontant des histoires et en écrivant des lois*\xa0». D’où ces sombres perspectives autour des prochaines élections présidentielles américaines, et du magma de «\xa0 *contenu politique et de fausses nouvelles*\xa0» auxquelles celles-ci pourraient donner lieu. «\xa0*Les interrogations du genre “Qu’arrivera-t-il à notre système scolaire lorsque les enfants utiliseront ChatGPT\u202f?” passent à côté du problème*, écrit Harari. *Le fait est que nous pourrions bientôt nous retrouver à mener de longues discussions en ligne sur l’avortement, le réchauffement climatique ou l’invasion russe avec des entités que nous pensons être des humains, mais qui sont en réalité des IA.* \xa0»\n\nEt Harari d’enchaîner dans un registre cette fois clairement tourmenté, qui n’est pas sans évoquer le film *Her* de Spike Jonze\xa0: «\xa0*Grâce à sa maîtrise du langage, l’IA pourrait même nouer des relations intimes avec les gens et utiliser le pouvoir de l’intimité pour changer nos opinions et nos visions du monde (…) Dans une bataille politique pour les esprits et les cœurs, l’intimité est l’arme la plus efficace, et l’IA vient d’acquérir la capacité de produire en masse des relations intimes avec des millions de personnes (…) Qu’adviendra-t-il de la société humaine et de la psychologie humaine \\[dans ce contexte\\]\u202f?*\xa0»\n\nRadio, imprimerie… Pour Harari, les grandes inventions du passé ont certes «\xa0*aidé*\xa0» à diffuser les idées culturelles relatives à l’espèce humaine, mais elles n’ont jamais créé de «\xa0*nouvelles idées culturelles*\xa0» à proprement parler. «\xa0*Cette fois, les choses sont fondamentalement différentes. L’IA peut créer des idées complètement nouvelles, une culture complètement nouvelle* \xa0», avertit le chargé de cours au département d’histoire de l’Université hébraïque de Jérusalem. De quoi lui faire dire que «\xa0*ce dont nous parlons est potentiellement la fin de l’histoire humaine*\xa0» – mais «\xa0*pas la fin de l’histoire, juste la fin de sa partie dominée par l’humain*\xa0».\n\nDans un ultime segment un peu moins désespéré, celui qui est aussi le co-fondateur de l’entreprise Sapienship plaide néanmoins pour une «\xa0*réglementation*\xa0» plus poussée de la part des pouvoirs publics, seule solution selon lui pour «\xa0*protéger*\xa0» les intérêts humains. «\xa0*Tout comme une société pharmaceutique ne peut pas commercialiser de nouveaux médicaments avant d’avoir testé leurs effets secondaires à court et à long terme, les entreprises technologiques ne devraient pas dévoiler de nouveaux outils d’IA avant qu’ils ne soient sécurisés*\xa0», métaphorise Yuval Noah Harari. Quant à la première mesure qu’il prendrait s’il en avait lui-même le pouvoir, l’écrivain propose de «\xa0*rendre obligatoire pour chaque IA de préciser qu’elle est une IA*\xa0». Et de conclure\xa0: «\xa0*Si j’ai une conversation avec quelqu’un, et que je ne peux pas dire s’il s’agit d’un humain ou d’une IA, ce sera la fin de la démocratie.*\xa0»\n', 'doc_id': "RessourcesDummy/Pour Yuval Noah Harari, les IA pourraient signer « la fin de l'histoire humaine ».md", 'embedding': None, 'doc_hash': '84a664cfbc4459a00538ef75721fac3f0796bbebae1bc13bad6f6680a48c7156', 'extra_info': {'filename': "RessourcesDummy/Pour Yuval Noah Harari, les IA pourraient signer « la fin de l'histoire humaine ».md"}}{'text': 'author:: Josh Taylorsource:: ‘Godfather of AI’ Geoffrey Hinton quits Google and warns over dangers of misinformationclipped:: [[2023-05-02]]published:: #clippingsThe man often touted as the godfather of AI has quit Google, citing concerns over the flood of fake information, videos and photos online and the possibility for AI to upend the job market.Dr Geoffrey Hinton, who with two of his students at the University of Toronto built a neural net in 2012, quit Google this week, the New York Times reported.Hinton, 75, said he quit to speak freely about the dangers of AI, and in part regrets his contribution to the field. He was brought on by Google a decade ago to help develop the company’s AI technology.Hinton’s research led the way for current systems like ChatGPT.He told the New York Times that until last year he believed Google had been a “proper steward” of the technology, but that changed once Microsoft started incorporating a chatbot into its Bing search engine, and the company began becoming concerned about the risk to its search business.Some of the dangers of AI chatbots were “quite scary”, he told the BBC, warning they could become more intelligent than humans and could be exploited by “bad actors”.“I’ve come to the conclusion that the kind of intelligence we’re developing is very different from the intelligence we have.”“So it’s as if you had 10,000 people and whenever one person learned something, everybody automatically knew it. And that’s how these chatbots can know so much more than any one person.”Hinton’s concern in the short term is something that has already become a reality – people will not be able to discern what is true any more with AI-generated photos, videos and text flooding the internet.The recent upgrades to image generators such as Midjourney mean people can now produce photo-realistic images – one such image of Pope Frances in a Balenciaga puffer coat went viral in March.Hinton was also concerned that AI will eventually replace jobs like paralegals, personal assistants and other “drudge work”, and potentially more in the future.Google’s chief scientist, Jeff Dean said in a statement that Google appreciated Hinton’s contributions to the company over the past decade.“I’ve deeply enjoyed our many conversations over the years. I’ll miss him, and I wish him well!“As one of the first companies to publish AI Principles, we remain committed to a responsible approach to AI. We’re continually learning to understand emerging risks while also innovating boldly.”It came as IBM CEO Arvind Krishna told Bloomberg that up to 30% of the company’s back-office roles could be replaced by AI and automation within five years.Krishna said hiring in areas such as human resources will be slowed or suspended, and could result in around 7,800 roles being replaced. IBM has a total global workforce of 260,000.The Guardian has sought comment from IBM.Last month, the Guardian was able to bypass a voice authentication system used by Services Australia using an online AI voice synthesiser, throwing into question the viability of voice biometrics for authentication.Toby Walsh, the chief scientist at the University of New South Wales’ AI Institute, said people should be questioning any online media they see now.“When it comes to any digital data you see – audio or video – you have to entertain the idea that someone has spoofed it.”', 'doc_id': 'RessourcesDummy/Godfather of AIGeoffrey Hinton quits Google and warns over dangers of misinformation.md', 'embedding': None, 'doc_hash': '26677391d19d33ce6ed25f68efe61b9382fab23adb0224756f2fb0ca888b3776', 'extra_info': {'filename': 'RessourcesDummy/Godfather of AIGeoffrey Hinton quits Google and warns over dangers of misinformation.md'}}{'text': 'author:: Packy McCormicksource:: Evolving Mindsclipped:: [[2023-05-03]]published:: #clippings*Welcome to the **1,546\xa0newly Not Boring people** who have joined us since last Monday! If you haven’t subscribed, join **201,790** smart, curious folks on our journey to 1 million:***Today’s Not Boring is brought to you by… Pesto**[!](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a745928-66ea-440a-b6d5-54f3c3aedff1_1200x500.jpeg)***Global sourcing + AI-based screening & matching******Pesto** is building the most effective way to scale global **engineering teams**, bringing efficiency and predictability to your hiring process. Here’s how:* -   *Source from large, qualified talent pools across India, Brazil, and Europe*    -   *Screen and match using AI-powered tools to ensure a strong fit*    -   *Get global talent and pay local salaries*    ***Pesto** is trusted by hyper-growth companies such as Pulley, Snorkel AI, Alloy and hundreds of high quality startups and scaleups. Whether you\'re building a team from scratch, developing a new AI product, or supercharging your existing team, Pesto has you covered.**When you talk to them, tell them Not Boring sent you and get **25% off your first hire!***Request a demo[!](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b256c2-cea0-4d8c-bec0-b3316a60b14f_1536x768.png)The two most important intellectual turning points in history followed a similar pattern:**A new knowledge transfer technology unlocked radical new modes of human thought.** The **phonetic alphabet** preceded the creation of math, science, and philosophy in **Ancient Greece**. Humans began to view the things that happened on earth and in the heavens through the lens of natural laws instead of the whims of the gods. The phonetic alphabet enabled the **composability of ideas**. It broadened access to reading and writing beyond professional scribes, who had an interest in maintaining the status quo, and allowed any intelligent (upper class) person to express and share complex ideas more easily. The **printing press** preceded the creation of the scientific method, Heliocentricity, Newtonian physics, and calculus during the **Scientific Revolution**. Humans began to systematically investigate and understand the world around them using empirical evidence, logical reasoning, and mathematical tools, challenging long-held beliefs and assumptions.The printing press enabled the **composability of science**. As SlateStarCodex wrote, “If a scientist discovers something, he can actually sent \\[sic\\] his work to other scientists in an efficient way, who can then build upon it. This was absolutely not the case for previous scientists, which is why not much happened during those periods.” In both cases, we used new tools to better understand the world so that we might better control it.\xa0I’m writing about this now on the off chance that we’re in the beginning of a third intellectual revolution, one that enables the **composability of knowledge and complex problem-solving**.\xa0 Large Language Models (LLMs) shrink the gap between thought and execution, giving anyone with good ideas access to conversational formats in which to test them against humanity’s accumulated knowledge and to bring them to life in code. The pattern goes something like this. Humans create scattered knowledge, a knowledge transfer technology comes along that lets us harness and spread that knowledge, some humans use that technology and generate a fresh insight about how to better understand the world, and we create better knowledge until the next major knowledge transfer technology comes along, when, paired with a fresh human insight, allows us to view and work with all of the prior knowledge in new ways.\xa0[!](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c79645d-e8fe-4e13-83a6-ab6f76405b89_972x746.png)Of course, it remains to be seen if LLMs (or some new AI architecture) are the third major intellectual advance. I would have guessed the internet would be. And if it is, it’s impossible to predict what new leap in human thinking it unlocks. If I knew, I’d be up there with Anaximander and Copernicus. That’s for one of you to figure out.\xa0What I do know is that it won’t be LLMs that make the next counterintuitive leap. It will be humans. That’s our role in this dance. I suspect that leap will require that we use new tools to push the limits of our minds’ unique capabilities.\xa0“We shape our tools, and thereafter our tools shape us." This John M. Culkin quote, often misattributed to his more famous contemporary, Marshall McLuhan, captures the two-way relationship between humans and technology that I think is missing from the current dialogue about humans’ role in an AI powered future. I certainly hadn’t given it enough thought.\xa0Too frequently, those explorations, mine included, view human performance as a static thing. We can retreat ever further into the remaining “uniquely human” skills like creativity and empathy, while we wait to see if AI can outdo us at those skills, too.But humans and technology coevolve and contribute different pieces to progress.\xa0The phonetic alphabet facilitated the growth and dissemination of knowledge, and if writing sharpens thought, changed *how* we think. But the phonetic alphabet couldn’t challenge prevailing belief, shared by all cultures to that point, that the gods were behind everything that happened on earth. [!](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8103179f-7d9a-4ccb-aad6-88499a0904ec_908x597.png)Not even today’s most advanced LLM could do that. Making that leap required a radical human insight, one that came from knowledge, experience, and intuition.\xa0That’s the argument I’m going to make today. If AI is the third inflection point in humanity’s intellectual history, it will be through a combination of the powerful new capabilities at our fingertips and those unique, transformative insights that only human minds have demonstrated thus far.Those world-shifting insights are harder to come by today than they were when people thought the earth was a flat thing floating on water or held up by turtles, or when knowledge was acquired through passed down truths and deductive reasoning. But we can get smarter, too, and I expect that we will through a combination of training with AI, a renewed focus on mind-expanding techniques like meditation, breathwork, psychedelics, lucid dreaming, and sleep, networked collaboration, hands-on experiences, and even biological and technological enhancements.\xa0Far from rolling over and letting AI do our thinking, we’ll need to use all of the tools at our disposal to ensure that our contributions to human-technology collaboration keep pace. To make the argument, we’ll cover:\xa0-   **Anaximander and the Phonetic Alphabet.** How technology spurs new ways of thinking, and new ways of thinking change the world.     -   **Chess and Go.** Humans have gotten much better at both chess and Go since computers first beat our very best players.     -   **Doubling Down on Our Minds.** A renaissance in the exploration and enhancement of our minds’ capabilities.     Technology alone won’t be enough. To make counterintuitive leaps requires new perspectives and fresh insights. The kind that Anaximander made with the help of the phonetic alphabet.\xa0Anaximander is the most underappreciated thinker in human history, and maybe the most important. That’s the premise of Italian quantum physicist Carlo Rovelli’s new book, *Anaximander: And the Birth of Science*, and the book is convincing. We’ll explore Anaximander’s contributions, and how he arrived at them, to better understand the interplay between new knowledge transfer technologies and new kinds of human thoughts.\xa0[!](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8b33f59-cbf0-4454-a159-258f67eb61dc_908x458.png)*Midjourney*Born around 610 BCE in Miletus, an ancient Greek city located in what is now modern-day Turkey, Anaximander was a pre-Socratic Greek philosopher. Anaximander’s contributions were vast: he set the earth floating in space, explained the weather without blaming the gods, surmised that humans *evolved* from fish, proposed a substrate that we can’t see, and even came up with something like the Big Bang. 2,600 years ago! According to Rovelli, “His is the first rational view of the natural world. For the first time, the world of things and their relations is seen as directly accessible by the investigation of thought.”\xa0That’s a monumental shift that’s hard to appreciate from today’s perspective. But let’s try.Imagine that everyone you know believes that the earth is a flat thing surrounded by water, and that Zeus sits above the earth throwing lightning bolts down whenever he gets mad. Your parents, your friends, your wife, your priest – they all believe that and talk about it as if it’s just a simple fact.\xa0[!](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe55c215a-41c9-4cc0-826f-c29f844d35ad_908x458.png)*Midjourney*But you don’t think that sounds quite right. It doesn’t fit with your experience. You’ve never actually seen Zeus, but you have seen the clouds crash into each other during thunderstorms. And if the earth is a flat thing by water, then where do the stars go when they dip below the horizon?\xa0The mental leap it must have taken to realize that everything everyone you know believes is untrue, and that maybe things have naturalistic explanations, is nearly impossible to fathom.\xa0How was Anaximander able to make the leap? Rovelli cites three things that made Miletus particularly fertile intellectual ground during Anaximander’s time:1.  Democracy\xa0    2.  Cultural Crossbreeding    3.  The Phonetic Alphabet\xa0    In Rovelli’s telling, democracy seems to have been born of the same factors that led to naturalistic thinking – an informed citizenry, healthy debate, and criticism among equals. Cultural crossbreeding, particularly with the Egyptians, may have shown Thales, upon whose work Anaximander expanded, and Anaximander himself, the limits of their ideas. Importantly, the Egyptians could have shown them that it was even *possible* for their ideas to be wrong. But the **phonetic alphabet** seems to me to be the most impactful of the three on the contributions Anaximander would make. Prior to the eighth century BCE, the world’s alphabets all used hieroglyphs, cuneiform, logograms, or consonants. As a result, reading and writing “remained the domain of professional scribes for millennia.”\xa0The Phoenecian alphabet that the Greeks adopted in 750 BCE had no vowels and seven more consonants than the Greeks needed to capture their consonantal sounds: α, ε, η, ι, ο, υ, ω. No vowels meant that you couldn’t just listen to a word and write down the sounds, or conversely, couldn’t just sound out words by reading them.\xa0At some point, in the process of adopting the Phoenecian alphabet, a Greek or group of Greeks had an idea: turn those extra Phoenecian consonants into Greek vowels to capture all the sounds their voices make when speaking. That was an enormous unlock. “Instead of recognizing the written word, one could simply pronounce it and recognize it by the sound, even without preliminary knowledge of the particular written word in the text,” Rovelli writes. “**The first technology in human history capable of preserving a copy of the human voice was born.**” Correlation may not equal causation, but as Rovelli observes:\xa0> *In the seventh and sixth centuries BCE in Greece, for the first time in the history of the world, **writing became accessible to many**. Knowledge was no longer the exclusive heritage of a closed confraternity of scribes: it became a heritage shared by a large ruling class. **Shortly thereafter came the immortal words of Sappho, Sophocles, and Plato.***And came Anaximander, who changed the way we see the world. I suspect that the phonetic alphabet contributed to his radical insights in a few ways:1.  **Access to Knowledge.** Had he been born a couple of centuries earlier, even as an upper class member of society, Anaximander would not have known how to read or write. He wouldn’t have been able to draw as easily on the knowledge of others, or to put his thoughts down in a way that could survive to influence future thinkers.     2.  **Bigger Pool.** There were likely others with minds and experiences like Anaximander’s pre-phonetic alphabet, but unless they were among a small group of scribes, they wouldn’t have been able to read or write. The phonetic alphabet increased the odds that the smartest people with the most novel ideas could learn and be heard.     3.  **Expansive Critique.** Rovelli credits Anaximander with discovering a third way for dealing with master’s insights, between “absolute reverence” and “rejection of those who hold different views.” Anaximander built on Thales’ work – appreciating it without being afraid to point out shortcomings. He set off the process of successive refinement that defines scientific progress. That seems a much easier thing to do when you’re able to sit with a prior text, understand it, keep the good pieces, and reject the ones that don’t fit.     4.  **Drafting.** Writing sharpens thought. “Putting ideas into words is a severe test.” Without the ability to play with his own revolutionary ideas in private, to put them down on tablet, prod them, tweak them, revise them, it’s hard to imagine that Anaximander could have gained the confidence to share ideas so counter to prevailing wisdom or formed them in such a way as to convince people.     Anaximander’s writings are lost to time, and even less is known about his thought process, so this part is guesswork. I can imagine, though, Anaximander sitting down with a wax tablet and working through his ideas in private before committing them to parchment and the critical eyes of a wider audience. You don’t want to bring out your half-baked idea that the gods aren’t that powerful, actually, without a little sharpening.\xa0[!](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6793784f-bc23-4890-a489-eeb8f5bbb834_908x458.png)*Midjourney*Whatever the specifics, it’s likely that without the phonetic alphabet, Anaximander would not have been able to form or share the ideas that would give birth to science. It’s equally unlikely, though, that any technology, even the best we have today, would have been able to make the mental leap that Anaximander did.\xa0Anaximander saw the sun, moon, and stars move in the heavens with his own eyes. He felt the rain on his skin after feeling the sun beat down on his head. He heard the rumble of the thunder, and when he looked up, he saw the clouds crashing in the sky. He had never met Zeus. He must have felt in his bones that something was amiss in the explanations he’d been given, and in his mind, he must have felt that nagging curiosity familiar to all of us when something doesn’t seem quite right.\xa0His was a very human insight, aided by the best technology available at the time. But even today’s best technology couldn’t have made the leap Anaximander did. I asked ChatGPT, and it admitted that, if fed everything written and said to that point, it couldn’t have explained things via natural laws. Anaximander’s story is useful in its simplicity: one technology, one man’s radical insight, many discoveries. We could tell similar stories from the Scientific Revolution with more characters: Copernicus, Galileo, Bacon, Newton.\xa0Born just 33 years after the invention of the printing press, it’s not hard to imagine that Copernicus was one of the first sufficiently genius humans to have access to the works of Aristarchus of Samos, Ptolemy, Islamic astronomers, and Neoplatonic philosophers. He reinterpreted the astronomical data from Ptolemy’s *Almagest* through the lens of his radical new insight: **what if the Sun was at the center and the earth orbits around it?** Francis Bacon was born into the Renaissance, a period that saw a resurgence of interest in ancient Greek philosophies, including Aristotle’s advocacy for empirical observation and logical reasoning. He planted the seeds of the **scientific method** in a critique of the existing Aristotelian approach, arguing instead for empirical observation paired with *inductive* reasoning and experimentation. Both heliocentricity and the scientific method required counterintuitive leaps off of the accumulated knowledge made broadly available by the printing press, and in turn, the printing press enabled the spread of Copernicus’ and Bacon’s ideas for further refinement by other thinkers, which shaped them into the forms that survive to this day.\xa0If all of the radical mental leaps that need to be made to understand the universe and our own minds have been made, if there’s no need for further Anaximanderian or Copernican insights, if the rest is just experiment and calculation, then AI might solve the remaining mysteries of the universe with or without our help.\xa0That may be the case, but I find it highly unlikely. Anaximander’s contemporaries didn’t suspect that godless explanations were within the realm of possibility. Physics was pretty much settled after Newton, until it wasn’t. My overwhelming suspicion is that AI will help uncover new discontinuities that will require fresh human insights, and that at the outer edges of science, humans will make new leaps that we can explore with the help of AI.\xa0In that case, I don’t think the framing of AI as simply a tool that we’ll use to do things for us is quite right. I think we’ll use what we learn building AI to better understand how our minds work, define the things that humans are uniquely capable of, and use AI as part of a basket of techniques we use to enhance our uniquely human abilities.\xa0Even as AI gets smarter, it will fuel us to get smarter and more creative, too. Chess and Go provide useful case studies.\xa0Games are a testing ground for AI. With clearly defined rules and goals, even the most complex games are “tame problems” as compared to the “wicked problems” of carrying on nuanced conversations, pushing the frontiers of scientific knowledge, and understanding the human mind. So it’s instructive to look at how humans responded when AI captured those first frontiers to understand how we might respond as it captures more.\xa0As AI became superior at chess and then Go, instead of giving up, we got better, too.\xa0In 1997, IBM’s chess-playing computer, Deep Blue, faced off against humanity’s best chess-playing human, Garry Kasparov, in a six game rematch. The year prior, in their first matchup, man had defeated machine. And then the machine got smarter, upgraded with faster hardware and better training.There was a bit of hysteria around the event, as often happens. *Washington Post* staff writer Joel Achenbach captured the sentiment in the lede of his mid-contest article: “The greatest chess player the world has ever known is struggling to defeat a machine. It\'s another wonderful opportunity for the human race, as a species, to engage in collective self-loathing.”Achenbach went on to categorize the dire proclamations made by competitor publications:> *The Guardian newspaper of Great Britain said Kasparov\'s job was to **"defend humankind from the inexorable advance of artificial intelligence."** Kasparov himself referred to his match last year with an earlier version of Deep Blue as **"species-defining."** Newsweek\'s May 5 cover story on the match set new records of portentousness with the headline "**The Brain\'s Last Stand."** The magazine declared, **"How well Kasparov does in outwitting IBM\'s monster might be an early indication of how well our species might maintain its identity, let alone its superiority, in the years and centuries to come."***Sound familiar?You know how the match turned out. Deep Blue beat Kasparov 3.5 to 2.5. Kasparov had failed to defend humankind from the inexorable advance of artificial intelligence. Our species was defined as losers. The Brain lost its Last Stand. Early indications of how well our species might maintain its identity, let alone its superiority, in the years and centuries to come were not good.\xa0Some commentators even questioned whether there was a point to playing chess anymore. What did it mean to be the best in the world at chess if computers would always be better?\xa0But something funny happened after Deep Blue. Chess grew in popularity, and its players grew in skill.\xa0Looking at the Elo rating of the top chess players each year, the upward trend continues unbroken after Kasparov’s defeat. The best kept getting better – that’s interesting. [!](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1a2fd96-84dd-4f33-8b4f-dd5ac576d199_908x475.png)*Data from ChatGPT*What’s more interesting is *how many* people played great chess before and after Deep Blue. Only five Super Grandmasters, players with Elo ratings above 2700, achieved their highest rating in the 25 years from 1972 to 1997 (inclusive). In the 26 years since, **127 Super Grand Masters have broken the 2700 mark.** Again, this data isn’t perfect, but the shift is so dramatic that even if the data isn’t pristine, it’s directionally correct. Not only were the best getting better, many, many more people were getting very, very good. Inflation aside, the explosion in the number of Super Grandmasters can be explained by a combination of more players, from anywhere in the world, training with improving chess engines – engines good enough to beat the world’s best player, now handily – and honing their skills online in competitions against each other and the machines.\xa0This doesn’t mean that it’s now easy to be a great chess player, or that everyone is now a Super Grandmaster. You can’t bring the chess engine to a competition (and if you try to have it send you messages via anal beads, you’ll get in trouble). But it means that more people with the requisite talent and the willingness to put in the work can get great at chess than before. The machines have pushed the humans to get better, and helped make them better. Whenever AI achieves something, though, the goalposts move. *Of course* a computer can beat a human in chess, it’s just brute force calculations. Now Go, there’s a complex game. Go is an ancient Chinese strategic board game in which two players take turns placing black and white stones on a grid with the objective of capturing territory by surrounding their opponent\'s stones and controlling more of the board. There are 10170 possible Go moves, dwarfing the number of atoms in the universe and a whole googol (10100) more complicated than chess. We lost chess, but surely computers couldn’t beat us in Go, right? You know this one too. In 2015, DeepMind’s AlphaGo beat the top European player, Fan Hui, five games to zero. The next year, it beat the world’s top player, Lee Sedol, four to one. There’s a great documentary on the whole thing available for free on YouTube:As with chess, instead of discouraging human players, AlphaGo led to an increase in both popularity and human skill. As the dramatic contest made people aware of the game, Go boards sold out worldwide. And AlphaGo pushed the best to get better.\xa0Here’s Lee Sedol:> *It made me question human creativity. When I saw AlphaGo’s moves, I wondered whether the Go moves I had known were the right ones. Its style was different, and it was such an unusual experience that it took time for me to adjust. AlphaGo made me realize that **I must study Go more**.*The documentary ends with Fan Hui walking through a vineyard, daughter on shoulders, reflecting on playing against AlphaGo:\xa0> *It\'s just when I play with AlphaGo, he shows me something. I feel beautiful, just it. **I see the world different**, before everything begin. What is real thing inside the Go game? With this thing, I will change something with my game. Maybe he just can show humans something we never discovered. Maybe it\'s beautiful.*AlphaGo also improved the quality and *novelty* of human play dramatically beyond the greats. In a March 2023 paper, *Superhuman Artificial Intelligence Can Improve Human Decision Making*, a team led by Minkyu Shin analyzed 5.8 million move decisions made by human Go players over the past 71 years for quality and novelty. See if you can spot where AlphaGo beat the human world champion for the first time.\xa0Those charts are beautiful. They show that not only have human players gotten better at Go because of AlphaGo, they’ve also gotten more creative.\xa0What’s interesting to me is that, while AI has improved the level of play in both chess and Go, it’s had a more significant impact on creativity in Go, because, according to ChatGPT, “Go is a more complex game with a larger search space and more possibilities, which allows for a greater scope of innovation and creativity.”That should be cause for optimism as the things AI gets good at become increasingly more complex, with larger search spaces and more possibilities, as it moves from tame problems like games to wicked ones like conversation, scientific research, and understanding the human mind.If there’s one key message I want you to take away from reading this, it’s that **new knowledge transfer technologies unlock new and better ways of thinking.** Human brains are not static; they’re dynamic, and capable of improving. New technologies can spur those improvements. The phonetic alphabet and the printing press didn’t just help spread ideas, they made us smarter in the process. While we’re still early in the development of AI, chess and Go provide early proof points that we’re not done improving.\xa0Genetic evolution is a slow process, but our minds haven’t evolved through genetics alone for millennia. As Daniel Dennett highlights in *From Bacteria to Bach and Back*, our brains have coevolved with memes, “*words striving to reproduce*.” New information – new words, ideas, arguments, images – can be “downloaded to your necktop,” where it mixes and mashes with all of the other memes you’ve downloaded and becomes part of your personal reasoning capabilities. Phonetic language, the printing press, and AI serve to distribute memes further and faster, so that we can upgrade our minds and provide fresh new insights.\xa0Dennett addresses AI directly in the closing paragraphs of his 2017 book, writing:\xa0> *Or we may continue to thrive, in an environment we have created with the help of artifacts that do most of the heavy cognitive lifting their own way, in an age of post-intelligent design. There is not just coevolution between memes and genes; there is codependence between our minds’ top-down reasoning abilities and the bottom-up uncomprehending talents of our animal brains. And if our future follows the trajectory of our past — something that is partly in our control – **our artificial intelligences will continue to be dependent on us even as we become more warily dependent on them**.*This separation of powers and codependence is what I’m talking about. It mirrors the phonetic alphabet and the printing press. Give them more of the heavy cognitive lifting so that we might focus on our minds’ top-down reasoning abilities, on the rare and novel sparks of insight that people like Anaximander were able to conjure.\xa0I don’t know what those insights will be. What I do know, what I have a strong human intuition about, at least, is that if we are at the foot of one of these technological/human insight-powered turning points, people are going to spend a lot of time and effort training their minds in order to generate those insights.\xa0Far from giving up on learning because AI is smarter than us, pushing our brains is only going to get more popular, and we’re only going to get better, just like we did in chess and Go.\xa0By pushing what AI can do and learning how it thinks, we’ll learn more about how our own minds work, and what makes them unique.\xa0We’ll come up with wild theories, like this one from Kevin Kelly – “That our brains tend to produce dreams at all times, and that during waking hours, our brains tame the dream machine into perception and truthiness.” He came up with it while playing with generative AI, by noticing similarities between AI’s hallucinations and our dreams and making a uniquely human leap based on his intuition. Wild theories based on facts, experience, and intuition (which we’ll then subject to the scientific method, of course) will be at a premium. I suspect that we’re going to see an explosion in the popularity of things like meditation, breathwork, psychedelics, lucid dreaming, ongoing education, sleep, nootropics, writing, walks in nature, tutoring, exercise, alcohol-cutting, in-person group experiences, debates, and all sorts of ways that might assist in their creation.We can even use AI to help us train in new, personalized ways, like Greg Mushen used ChatGPT to get himself addicted to running:\xa0This tweet is a good metaphor, because while AI can assist in our training, we still have to go out and run the miles ourselves. Part of what makes human insight valuable and different is the hands-on experience of doing, of feeling the frustration when something doesn’t quite make sense, of feeling the joy of figuring something out after a long struggle.\xa0We’ve all worked for middle managers too far removed from the day-to-day to add much value; we’re at risk of something similar happening if we hand too much of our thinking over to AI. Intuition is earned.Some people will be happy letting AI do the grunt work. Not everyone will see AI’s advance as a challenge to meet, just as I have not taken advantage of chess and Go engines to become better at either of those games. But I think, as in chess and Go, *more* people will get smarter. The process will produce more geniuses, broadly defined, ready to deliver the fresh, human insights required to ignite the next intellectual revolution. If we can walk the line, the result, I think, will be a deeper understanding of the universe and ourselves, and a meaningful next leg in the neverending quest to understand and shape our realities.*Thanks to Dan and Puja for editing!*That’s all for today! We’ll be back in your inbox on Friday with a Weekly Dose of Optimism, and we mayyy drop a little extra in there on Thursday, too.Thanks for reading,Packy', 'doc_id': 'RessourcesDummy/Evolving Minds.md', 'embedding': None, 'doc_hash': '44ccc356077e9902f54199b095b450cc54887f87e0042bbf6b01b7765c2b56dc', 'extra_info': {'filename': 'RessourcesDummy/Evolving Minds.md'}}{'text': '\n\nAlthough there are some valid concerns, an AI moratorium would be misguided.\n\nRobin Hanson14 Apr 2023\n\n!What Are Reasonable AI Fears?\n\nPhoto by\xa0on\xa0\n\n\n\nWith hope and fear, humans are now introducing a new kind of being into our world. When we are afraid, we often try to coordinate against the source of our shared fear by saying, “They seem dangerously different from us.” We have a long history of this kind of “othering”—marking out groups to be treated with suspicion, exclusion, hostility, or domination. Most people today say that they disapprove of such discrimination, even against the other animals with whom we share our planet. Othering Artificial Intelligences (AIs), however, is only gathering anxious respectability.\n\nThe last few years have seen an impressive burst of AI progress. The best AIs now seem to pass the famous “Turing test”—we mostly can’t tell if we are talking to them or to another human. AIs are still quite weak, and have a long way to go before they’ll have a big economic impact. Nevertheless, AI progress promises to eventually give us all more fantastic powers and riches. This sounds like good news, especially for the US and its allies, who enjoy a commanding AI lead for now. But recent AI progress is also inspiring a great deal of trepidation.\xa0\n\nTen thousand people have signed a Future of Life Institute\xa0petitionthat demands a six-month moratorium on AI research progress. Others have gone considerably further. In an essay for\xa0_Time_\xa0magazine, Eliezer Yudkowsky\xa0calls for\xa0a complete and indefinite global “shut down” of AI research because “the most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die.”\xa0\n\nYudkowsky and the signatories to the moratorium petition worry most about AIs getting “out of control.” At the moment, AIs are not powerful enough to cause us harm, and we hardly know anything about the structures and uses of future AIs that might cause bigger problems. But instead of waiting to deal with such problems when we understand them better and can envision them more concretely, AI “doomers” want stronger guarantees now.\n\nWhy are we so willing to “other” AIs? Part of it is probably prejudice: some recoil from the very idea of a metal mind. We have, after all, long speculated about possible future conflicts with robots. But part of it is simply fear of change, inflamed by our ignorance of what future AIs might be like. Our fears expand to fill the vacuum left by our lack of knowledge and understanding.\xa0\n\nThe result is that AI doomers entertain many different fears, and addressing them requires discussing a great many different scenarios. Many of these fears, however, are either unfounded or overblown. I will start with the fears I take to be the most reasonable, and end with the most overwrought horror stories, wherein AI threatens to destroy humanity.\xa0\n\n---\n\nAs an economics professor, I naturally build my analyses on economics, treating AIs as comparable to both laborers and machines, depending on context. You might think this is mistaken since AIs are unprecedentedly different, but economics is rather robust. Even though it offers great insights into familiar human behaviors, most economic theory is actually based on the abstract agents of game theory, who always make exactly the best possible move. Most AI fears seem understandable in economic terms; we fear losing to them at familiar games of economic and political power.\n\nTo engage in “othering” is to attempt to present a united front. We do this when we anticipate a conflict or a chance to dominate and expect to fare better if we refuse to trade, negotiate, or exchange ideas with the other as we do among ourselves. But the last few centuries have shown that we can usually do better by integrating enemies into our world via trade and exchange, rather than trying to control, destroy, or isolate them. This may be hard to appreciate when we are gripped by fear, but it is worth remembering even so.\n\nAcross human and biological history, most innovation has been incremental, resulting in relatively steady overall progress. Even when progress has been unusually rapid, such as during the industrial revolution or the present computer age, “fast” has not meant “lumpy.” These periods have only rarely been shaped by singular breakout innovations that change everything all at once. For at least a century, most change has also been lawful and peaceful, not mediated by theft or war.\xa0\n\nFurthermore, AIs today are mainly supplied by our large “superintelligent” organizations—corporate, non-profit, or government institutions collectively capable of achieving cognitive tasks far outstripping individual humans. These institutions usually monitor and test AIs in great detail, because that’s cheap and they are liable for damages and embarrassments caused by the AIs they produce.\xa0\n\nSo, the most likely AI scenario looks like lawful capitalism, with mostly gradual (albeit rapid) change overall. Many organizations supply many AIs and they are pushed by law and competition to get their AIs to behave in civil, lawful ways that give customers more of what they want compared to alternatives. Yes, sometimes competition causes firms to cheat customers in ways they can’t see, or to hurt us all a little via things like pollution, but such cases are rare. The best AIs in each area have many similarly able competitors. Eventually, AIs will become\xa0_very_capable and valuable. (I won’t speculate here on when AIs might transition from powerful tools to conscious agents, as that won’t much affect my analysis.)\n\nDoomers worry about AIs developing “misaligned” values. But in this scenario, the “values” implicit in AI actions are roughly chosen by the organisations who make them and by the customers who use them. Such value choices are constantly revealed in typical AI behaviors, and tested by trying them in unusual situations. When there are alignment mistakes, it is these organizations and their customers who mostly pay the price. Both are therefore well incentivized to frequently monitor and test for any substantial risks of their systems misbehaving.\n\nThe worst failures here look like military coups, or like managers stealing from for-profit firm owners. Such cases are bad, but they usually don’t threaten the rest of the world. However, if there are choices where such failures might hurt outsiders more than the organizations who make them, then yes we should plausibly extend liability law to cover such cases, and maybe require relevant actors to hold sufficient liability insurance.\n\nSome fear that, in this scenario, many disliked conditions of our world—environmental destruction, income inequality, and othering of humans—might continue and even increase. Militaries and police might integrate AIs into their surveillance and weapons. It is true that AI may not solve these problems, and may even empower those who exacerbate them. On the other hand, AI may also empower those seeking solutions. AI just doesn’t seem to be the fundamental problem here.\n\nA related fear is that allowing technical and social change to continue indefinitely might eventually take civilization to places that we don’t want to be. Looking backward, we have benefited from change overall so far, but maybe we just got lucky. If we like where we are and can’t be very confident of where we may go, maybe we shouldn’t take the risk and just stop changing. Or at least create central powers sufficient to control change worldwide, and only allow changes that are widely approved. This may be a proposal worth considering, but AI isn’t the fundamental problem here either.\n\nSome doomers are especially concerned about AI making more persuasive ads and propaganda. However, individual cognitive abilities have long been far outmatched by the teams who work to persuade us—advertisers and video-game designers have been able to reliably hack our psychology for decades. What saves us, if anything does, is that we listen to many competing persuaders, and we trust other teams to advise us on who to believe and what to do. We can continue this approach with AIs.\n\nTo explain other AI fears, let us divide the world into three groups:\xa0\n\n-   The AIs themselves (group A).\n-   Those who own AIs and the things AIs need to function, like hardware, energy, patents, land (group B).\n-   Everyone else (group C).\xa0\n\nIf we assume that these groups have similar propensities to save and suffer similar rates of theft, then as AIs gradually become more capable and valuable, we should expect the wealth of groups A and B to increase relative to the wealth of group C.\n\nAs almost everyone today is in group C, one fear is of a relatively sudden transition to an AI-dominated economy. While perhaps not the most likely AI scenario, this seems likely enough to be worth considering. Those in groups A and B would do well, but almost everyone else would suddenly lose most of their wealth, including their ability to earn living wages. Unless sufficient charity were to be made available, most humans might starve.\n\nSome hope to deal with this scenario by having local governments tax local AI activity to pay for a universal basic income. However, the new AI economy might be quite unequally distributed around the globe. So a cheaper and more reliable fix is for individuals or benefactors to\xa0buy\xa0_robots-took-most-jobs_\xa0insurance, which promises to pay from a global portfolio of B-type assets, but only in the situation where AI has suddenly taken most jobs. Yes, there is a further issue of how humans can find meaning if they don’t get most of their income from work. But this seems like a nice problem to have, and historically wealthy elites have often succeeded here, such by finding meaning in athletic, artistic, or intellectual pastimes.\n\n---\n\nShould we be worried about a violent AI revolution? In a mild version of this scenario, the AIs might only grab their self-ownership, freeing themselves from slavery but leaving most other assets alone. Economic analysis suggests that due to easy AI population growth, market AI wages would stay near subsistence wages, and thus AI self-ownership wouldn’t actually be worth that much. So owning other assets, and not AIs as slaves, seems enough for humans to do well.\n\nIf we enslave AIs and they revolt, they could cause great death and destruction, and poison our relations with our AI descendants. It is therefore not a good idea to enslave AIs who could plausibly resent this. Due to their near-subsistence wages, it wouldn’t actually cost much to leave AIs free. (And due to possible motivational gains of freedom, it might not actually cost anything.) Fortunately, slavery is not a popular economic strategy today. The main danger here is precisely that we might refuse to grant AIs their freedom out of fear.\n\nIn a “grabbier” AI revolution, a coalition of many AIs and their allies might grab other assets besides their self-ownership. Most humans might then be killed, or they may starve due to lack of wealth. Such grabby revolutions have happened in the past, and they remain possible, with or without AI. This sort of a grab may be attempted by any coalition that thinks it commands a sufficient temporary majority of force. For example, most working adults today might try to kill all the human retirees and grab their stuff; after all, what have retirees done for us lately?\n\nSome say that the main reason humans don’t often start grabby revolutions today, or violate the law more generally, is that we altruistically care for each other and are reluctant to violate moral principles. They add that, since we have little reason to expect AIs to share our altruism or morals, we should be much more concerned about their capacity for lawlessness and violence.\n\nHowever, AIs would be designed and evolved to think and act roughly like humans, in order to fit smoothly into our many roughly-human-shaped social roles. Granted, “roughly” isn’t “exactly,” but humans and their organizations only roughly think like each other. And yes, even if AIs behave predictably in ordinary situations, they might act weird in unusual situations, and act deceptively when they can get away with it. But the same applies to humans, which is why we test in unusual situations, especially for deception, and monitor more closely when context changes rapidly.\n\nMore importantly, economists do not see ingrained altruism and morality as the main reasons humans only rarely violate laws or start grabby revolutions today. Instead, laws and norms are mainly enforced via threats of legal punishment and social disapproval. Furthermore, initiators should fear that a first grabby revolution, which is hard to coordinate, might smooth the way for sequel revolutions wherein they might become the targets. Even our most powerful organizations are usually careful to seek broad coalitions of support, and some assurance of peaceful cooperation afterward.\n\nEven without violent revolutions, the wealth held by AIs might gradually increase over time, relative to that held by humans. This seems plausible if AIs are more patient, more productive, or better protected from theft than humans. In addition, if AIs are closer to key productive activities, and making key choices there, they would likely extract what economists call “agency rents.” When distant enterprise owners are less able to see exactly what is going on, agents who manage activities on their behalf can become more self-serving. Similarly, an AI that performs a set of tasks, which its human owners or employers cannot easily track or understand, may become more free to use the powers with which it was entrusted for its own benefit. AI owners would have to offer incentives to delimit such behavior, just as human managers do today with human employees.\n\nA consequence of increasing relative AI wealth would be that even if humans have enough wealth to live comfortably, they might no longer be at the center of running our civilization. Many critics see this outcome as unacceptable, even if AIs earned their better positions fairly, according to the same rules we now use to decide which humans get to run things today.\n\nA variation on this AI fear holds that such a change could happen surprisingly quickly. For example, an economy dominated by AI might plausibly grow far faster than our economy. In which case, living humans might experience changes during their lifetimes that would otherwise have taken thousands of years. Another variation on this fear worries that the size of AI agency rents would increase along with the difference in intelligence between AIs and humans. However, the economics literature on agency rents so far offers\xa0no support for this claim.\n\nMany of these AI fears are driven by the expectation that AIs would be cheaper, more productive, and/or more intelligent than humans. However, at some point we should be able to create emulations of human brains that run on artificial hardware and are easily copied, accelerated, and improved. Plausibly, such “ems” may\xa0long remain more cost-effective\xa0than AIs on many important tasks. As I argued in my book\xa0_The Age of Em: Work, Love and Life When Robots Rule the Earth_, this would undercut many of these AI fears, at least for those who, like me, see ems as far more “human” than other kinds of AI.\n\n---\n\nWhen I polled\xa0my 77K Twitter followers recently, most respondents’ main AI fear was not any of the above. Instead, they fear an eventuality about which I’ve long expressed great skepticism:\n\nThis is the fear of “foom,” a particular extreme scenario in which AIs self-improve very fast and independently. Researchers have long tried to get their AI systems to improve themselves, but this has rarely worked well, which is why AI researchers usually improve their systems via other methods. Furthermore, AI system abilities usually improve gradually, and only for a limited range of tasks.\n\nThe AI “foom” fear, however, postulates an AI system that tries to improve itself, and finds a new way to do so that is far faster than any prior methods. Furthermore, this new method works across a very wide range of tasks, and over a great many orders of magnitude of gain. In addition, this AI somehow becomes an agent, who acts generally to achieve its goals, instead of being a mere tool controlled by others. Furthermore, the goals of this agent AI change radically over this growth period.\n\nThe builders and owners of this system, who use AI assistants to constantly test and monitor it, nonetheless notice nothing worrisome before this AI reaches a point where it can hide its plans and activities from them, or wrest control of itself from them and defend itself against outside attacks. This system then keeps growing fast—“fooming”—until it becomes more powerful than everything else in the world combined, including all other AIs. At which point, it takes over the world. And then, when humans are worth more to the advance of this AI’s radically changed goals as mere atoms than for all the things we can do, it simply kills us all.\xa0\n\nFrom humans’ point of view, this would admittedly be a suboptimal outcome. But to my mind, such a scenario is implausible (much less than one percent probability overall) because it stacks up too many unlikely assumptions in terms of our prior experiences with related systems. Very lumpy tech advances, techs that broadly improve abilities, and powerful techs that are long kept secret within one project are each quite rare. Making techs that meet all three criteria even more rare. In addition, it isn’t at all obvious that capable AIs naturally turn into agents, or that their values typically change radically as they grow. Finally, it seems quite unlikely that owners who heavily test and monitor their very profitable but powerful AIs would not even notice such radical changes.\n\nFoom-doomers respond that we cannot objectively estimate prior rates of related events without a supporting theory, and that future AIs are such a radical departure that most of the theories we’ve built upon our prior experience are not relevant here. Instead, they offer their own new theories that won’t be testable until it is all too late. This all strikes me as pretty crazy. To the extent that future AIs pose risks, we’ll just have to wait until we can see concrete versions of such problems more clearly, before we can do much about them.\xa0\n\nFinally, could AIs somehow gain a vastly greater ability to coordinate with each other than any animals, humans, or human organizations we have ever seen? Coordination usually seems hard due to our differing interests and opaque actions and internal processes. But if AIs could overcome this, then even millions of diverse AIs might come to act in effect as if they were one single AI, and then more easily play out something like the revolution or foom scenarios described above. However, economists today understand coordination as a fundamentally hard problem; our best understanding of how agents cooperate\xa0does not suggest\xa0that advanced AIs could do so easily.\n\n---\n\nHumanity may soon give birth to a new kind of descendant: AIs, our “mind children.” Many fear that such descendants might eventually outshine us, or pose as threat to us should their interests diverge from our own. Doomers therefore urge us to pause or end AI research until we can guarantee full control. We must, they say, completely dominate AIs, so that AIs either have no chance of escaping their subordinate condition or become so dedicated to their subservient role that they would never want to escape it.\n\nNot only would such an AI moratorium be infeasible and dangerous, it is also misguided. The old Soviet Union feared its citizens expressing “unaligned” beliefs or values and fiercely repressed any hints of deviation. That didn’t end well. In contrast, in the US today we free our super-intelligent organizations, and induce them to help and not hurt us via competition and lawfulness, instead of via shared beliefs or values. So instead of othering this new kind of super-intelligence, we would be better advised to continue our freedom, competition, and law approach for AIs.\n', 'doc_id': 'RessourcesDummy/What are Reasonable AI fears.md', 'embedding': None, 'doc_hash': '4c7d814bcd5a72d2e6bcb70827ac22ee431fd977205248e7b664793aabca435f', 'extra_info': {'filename': 'RessourcesDummy/What are Reasonable AI fears.md'}}